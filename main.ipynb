{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, os, random, torch, torchvision, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = number of data points (files) |{x^(i), y^(i)}i=1->N|\n",
    "# x^(i)=[x_1^(i),...] a bag of characters\n",
    "# M_i = number of characters in x_i\n",
    "# S = set of all character types (a,b,c...)\n",
    "# K_S = |S|\n",
    "# L = set of data labels (e,s,j)\n",
    "# K_L = |L|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name is the name of the file eg 'e0.txt'\n",
    "# good_chars is a string of accepted characters\n",
    "# returns list of good chars in the file\n",
    "def read_file(name, good_chars=string.ascii_lowercase+\" \"):\n",
    "    # init character list\n",
    "    char_list = []\n",
    "    # open file\n",
    "    with open(f\"languageID/{name}\") as f:\n",
    "        # look at each file\n",
    "        for char in f.read():\n",
    "            # check if the character is approved, if so append it\n",
    "            if char in good_chars:\n",
    "                char_list.append(char)\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 formula / fxn\n",
    "def prior(data, target, alpha, Kl=3):\n",
    "    num = alpha\n",
    "    for item in data:\n",
    "        if item == target:\n",
    "            num += 1\n",
    "    return num / (len(data) + (Kl * alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files x0.txt-x9.txt for x = e,s,j\n",
    "train_data = []\n",
    "for name in os.listdir(\"languageID\"):\n",
    "    # avoid scrambled file\n",
    "    if len(name) < 8:\n",
    "        if int(name[1:-4]) <= 9:\n",
    "            train_data.append((name, read_file(name)))\n",
    "p1_list = [tup[0][0] for tup in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 0.3333333333333333\n",
      "j: 0.3333333333333333\n",
      "s: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# 2.1 priors\n",
    "prior_e = prior(p1_list, \"e\", alpha=0.5)\n",
    "print(\"e:\", prior_e)\n",
    "prior_j = prior(p1_list, \"j\", alpha=0.5)\n",
    "print(\"j:\", prior_j)\n",
    "prior_s = prior(p1_list, \"s\", alpha=0.5)\n",
    "print(\"s:\", prior_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string of good characters\n",
    "big_string = string.ascii_lowercase+\" \"\n",
    "# dict[letter]->number i.e. a->0, b->1, ...\n",
    "let_to_num = {}\n",
    "for i in range(27):\n",
    "    letter = big_string[i]\n",
    "    let_to_num[letter] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_prob(data, y_targ, alpha=0.5):\n",
    "    # init thetas\n",
    "    theta_nums = np.zeros(27) + alpha\n",
    "    theta_dens = np.zeros(27)\n",
    "    # init S\n",
    "    S = []\n",
    "    # loops through [(e0.txt, [a, b, ...]), (e1.txt, [...]), ...]\n",
    "    for tup in data:\n",
    "        # tup[0] is the file name\n",
    "        # tup[1] is the list of chars\n",
    "        # if the file is the target file\n",
    "        if tup[0][0] == y_targ:\n",
    "            # update S\n",
    "            S = list(set(S + tup[1]))\n",
    "            # loop through characters in the file\n",
    "            for char in tup[1]:\n",
    "                # get idx\n",
    "                idx = let_to_num[char]\n",
    "                # update params\n",
    "                theta_nums[idx] += 1\n",
    "            theta_dens += len(tup[1])\n",
    "    return theta_nums / (theta_dens + (len(S)*alpha))\n",
    "\n",
    "# make p(x)\n",
    "def cond_prob_all(data, alpha=0.5):\n",
    "    # init thetas\n",
    "    theta_nums = np.zeros(27) + alpha\n",
    "    theta_dens = np.zeros(27)\n",
    "    # init S\n",
    "    S = []\n",
    "    # loops through [(e0.txt, [a, b, ...]), (e1.txt, [...]), ...]\n",
    "    for tup in data:\n",
    "        # tup[0] is the file name\n",
    "        # tup[1] is the list of chars\n",
    "        # ignore scrambled file\n",
    "        if len(tup[0]) < 8:\n",
    "            # update S\n",
    "            S = list(set(S + tup[1]))\n",
    "            # loop through characters in the file\n",
    "            for char in tup[1]:\n",
    "                # get idx\n",
    "                idx = let_to_num[char]\n",
    "                # update params\n",
    "                theta_nums[idx] += 1\n",
    "            theta_dens += len(tup[1])\n",
    "    return theta_nums / (theta_dens + (len(S)*alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06016851, 0.01113497, 0.02151   , 0.02197258, 0.10536924,\n",
       "       0.01893276, 0.01747894, 0.04721626, 0.05541054, 0.00142078,\n",
       "       0.00373369, 0.02897737, 0.02051875, 0.05792169, 0.0644639 ,\n",
       "       0.01675202, 0.0005617 , 0.05382455, 0.06618206, 0.08012556,\n",
       "       0.02666446, 0.00928465, 0.01549645, 0.00115645, 0.01384437,\n",
       "       0.00062779, 0.17924996])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2\n",
    "theta_e = cond_prob(train_data, \"e\")\n",
    "theta_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.31770215e-01, 1.08672863e-02, 5.48605773e-03, 1.72269201e-02,\n",
       "       6.02068628e-02, 3.87867776e-03, 1.40121602e-02, 3.17632259e-02,\n",
       "       9.70368300e-02, 2.34118387e-03, 5.74114194e-02, 1.43266476e-03,\n",
       "       3.98001258e-02, 5.67125585e-02, 9.11663988e-02, 8.73576071e-04,\n",
       "       1.04829129e-04, 4.28052275e-02, 4.21762527e-02, 5.69921029e-02,\n",
       "       7.06198896e-02, 2.44601300e-04, 1.97428192e-02, 3.49430428e-05,\n",
       "       1.41519324e-02, 7.72241247e-03, 1.23453770e-01])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3\n",
    "theta_j = cond_prob(train_data, \"j\")\n",
    "theta_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04560451e-01, 8.23286362e-03, 3.75258241e-02, 3.97459221e-02,\n",
       "       1.13810860e-01, 8.60287996e-03, 7.18448398e-03, 4.53270019e-03,\n",
       "       4.98597021e-02, 6.62945947e-03, 2.77512257e-04, 5.29431717e-02,\n",
       "       2.58086399e-02, 5.41765595e-02, 7.24923684e-02, 2.42669051e-02,\n",
       "       7.67783910e-03, 5.92951189e-02, 6.57704049e-02, 3.56140730e-02,\n",
       "       3.37023219e-02, 5.88942678e-03, 9.25040856e-05, 2.49761031e-03,\n",
       "       7.86284728e-03, 2.68261848e-03, 1.68264932e-01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3\n",
    "theta_s = cond_prob(train_data, \"s\")\n",
    "theta_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name is the name of the file \"e10.txt\"\n",
    "# returns frequency list \n",
    "def get_theta(name):\n",
    "    theta = np.zeros(27)\n",
    "    for char in read_file(name):\n",
    "        theta[let_to_num[char]] += 1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([164.,  32.,  53.,  57., 311.,  55.,  51., 140., 140.,   3.,   6.,\n",
       "        85.,  64., 139., 182.,  53.,   3., 141., 186., 225.,  65.,  31.,\n",
       "        47.,   4.,  38.,   2., 498.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4\n",
    "x_e10 = get_theta(\"e10.txt\")\n",
    "x_e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x_given_y(x_list, theta_list):\n",
    "    tot = 0\n",
    "    for i in np.arange(len(x_list)):\n",
    "        tot += x_list[i] * np.log(theta_list[i])\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi(n, k_list):\n",
    "    num = factorial(n)\n",
    "    den = 1\n",
    "    for k in k_list:\n",
    "        den *= factorial(k)\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_of_x(x_vec, theta_list):\n",
    "    tot = 0\n",
    "    for i in range(len(x_vec)):\n",
    "        tot += x_vec[i] * np.log(theta_list[i])\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8001.2162734346075"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_all = cond_prob_all(train_data)\n",
    "px = p_of_x(x_e10, theta_all)\n",
    "px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7841.865447060635\n",
      "-8467.282044010557\n",
      "-8771.336113825271\n"
     ]
    }
   ],
   "source": [
    "# 2.5\n",
    "px_e = p_x_given_y(x_e10, theta_e)\n",
    "print(px_e)\n",
    "px_s = p_x_given_y(x_e10, theta_s)\n",
    "print(px_s)\n",
    "px_j = p_x_given_y(x_e10, theta_j)\n",
    "print(px_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_e = p_x_given_y(x_e10, theta_e)\n",
    "like_j = p_x_given_y(x_e10, theta_j)\n",
    "like_s = p_x_given_y(x_e10, theta_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 158.25221408530433\n",
      "j: -771.2184526793317\n",
      "s: -467.16438286461744\n",
      "=> e\n"
     ]
    }
   ],
   "source": [
    "# 2.6\n",
    "print(\"e:\", np.log(prior_e) + like_e - px)\n",
    "print(\"j:\", np.log(prior_j) + like_j - px)\n",
    "print(\"s:\", np.log(prior_s) + like_s - px)\n",
    "print(\"=> e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files x10.txt-x19.txt for x = e,s,j\n",
    "test_list = []\n",
    "for name in os.listdir(\"languageID\"):\n",
    "    # avoid scrambled file\n",
    "    if len(name) < 8:\n",
    "        if int(name[1:-4]) > 9:\n",
    "            test_list.append((name, read_file(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English (true)</th>\n",
       "      <th>Spanish (true)</th>\n",
       "      <th>Japanese (true)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English (pred)</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish (pred)</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese (pred)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 English (true)  Spanish (true)  Japanese (true)\n",
       "English (pred)               10               0                0\n",
       "Spanish (pred)                0              10                0\n",
       "Japanese (pred)               0               0               10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init a confusion matrix as a df\n",
    "conf_dict = {}\n",
    "langs = [\"English\", \"Spanish\", \"Japanese\"]\n",
    "for real_lang in langs:\n",
    "    conf_dict[real_lang + \" (true)\"] = {}\n",
    "    for pred_lang in langs:\n",
    "        conf_dict[real_lang + \" (true)\"][pred_lang + \" (pred)\"] = 0\n",
    "conf_df = pd.DataFrame(conf_dict)\n",
    "\n",
    "# make prior val and theta lists\n",
    "lang_list = ['e', 's', 'j']\n",
    "pri_dict = {}\n",
    "theta_dict = {}\n",
    "for lang in lang_list:\n",
    "    pri_dict[lang] = np.log(prior(test_list, lang, alpha=0.5))\n",
    "    theta_dict[lang] = cond_prob(test_list, lang)\n",
    "\n",
    "# loop through data, calc x and make pred\n",
    "for tup in test_list:\n",
    "    # get the x vector\n",
    "    x_vec = get_theta(tup[0])\n",
    "    \n",
    "    # find the probabilities\n",
    "    prob_e = p_x_given_y(x_vec, theta_dict['e']) + pri_dict['e'] - px\n",
    "    prob_s = p_x_given_y(x_vec, theta_dict['s']) + pri_dict['s'] - px\n",
    "    prob_j = p_x_given_y(x_vec, theta_dict['j']) + pri_dict['j'] - px\n",
    "        \n",
    "    # see which likelihood is the greatest\n",
    "    if prob_e >= prob_s and prob_e >= prob_j:\n",
    "        pred = \"English (pred)\"\n",
    "    if prob_s > prob_e and prob_s >= prob_j:\n",
    "        pred = \"Spanish (pred)\"\n",
    "    if prob_j > prob_s and prob_j >= prob_e:\n",
    "        pred = \"Japanese (pred)\"\n",
    "    \n",
    "    # find the true label\n",
    "    lab = tup[0][0]\n",
    "    if lab == 'e':\n",
    "        true = \"English (true)\"\n",
    "    if lab == 's':\n",
    "        true = \"Spanish (true)\"\n",
    "    if lab == 'j':\n",
    "        true = \"Japanese (true)\"\n",
    "    \n",
    "    # update the confusion matrix\n",
    "    conf_df[true][pred] += 1\n",
    "\n",
    "# view the confusion matrix\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a file\n",
    "unscram = read_file(\"s15.txt\")\n",
    "# shuffle it\n",
    "random.shuffle(unscram)\n",
    "# write the shuffled file\n",
    "scram = \"\".join(unscram)\n",
    "with open(\"languageID\\s15SCRAM.txt\", \"w\") as f:\n",
    "    f.write(scram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish (pred)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the x vector\n",
    "x_vec = get_theta(\"s15SCRAM.txt\")\n",
    "\n",
    "# get p(x)\n",
    "px_scram = p_of_x(x_vec, theta_all)\n",
    "\n",
    "# find the probabilities\n",
    "prob_e = p_x_given_y(x_vec, theta_dict['e']) + pri_dict['e'] - px_scram\n",
    "prob_s = p_x_given_y(x_vec, theta_dict['s']) + pri_dict['s'] - px_scram\n",
    "prob_j = p_x_given_y(x_vec, theta_dict['j']) + pri_dict['j'] - px_scram\n",
    "\n",
    "# see which likelihood is the greatest\n",
    "if prob_e >= prob_s and prob_e >= prob_j:\n",
    "    pred = \"English (pred)\"\n",
    "if prob_s > prob_e and prob_s >= prob_j:\n",
    "    pred = \"Spanish (pred)\"\n",
    "if prob_j > prob_s and prob_j >= prob_e:\n",
    "    pred = \"Japanese (pred)\"\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1+np.exp(-z))**(-1)\n",
    "\n",
    "def soft_max(z_list):\n",
    "    exp = np.exp(z_list)\n",
    "    return exp / sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SGD(train_x_list, train_y_list, d, alpha, batch_size, epochs, is_zero_w=False, d1=300, d2=200, k=10):\n",
    "    # init weights\n",
    "    if not is_zero_w:\n",
    "        W1 = np.random.uniform(-1, 1, (d1,d)) / d\n",
    "        W2 = np.random.uniform(-1, 1, (d2, d1)) / d1\n",
    "        W3 = np.random.uniform(-1, 1, (k, d2)) / d2\n",
    "    else:\n",
    "        W1 = np.zeros(d1,d)\n",
    "        W2 = np.zeros(d2, d1)\n",
    "        W3 = np.zeros(k, d2)\n",
    "    # check stopping criteria\n",
    "    for run in range(epochs):\n",
    "        # sample training pts\n",
    "        idx = np.random.randint(0, len(train_list))\n",
    "        x = train_list[idx]\n",
    "        y = train_y_list[0]\n",
    "        \n",
    "        a1 = W1 @ x\n",
    "        a2 = W2 @ sigmoid(a1)\n",
    "        a3 = W3 @ sigmoid(a2)\n",
    "        \n",
    "        # compute f(network)\n",
    "        yhat = soft_max(a3)\n",
    "        \n",
    "        # compute gradients\n",
    "        # for W3\n",
    "        dldg = -y / yhat\n",
    "        dgda3 = y * np.eye(k) - np.outer(y, y)\n",
    "        \n",
    "        # for W2\n",
    "        sig2 = W2 @ sigmoid(a1)\n",
    "        dsigda2 = np.outer(sig2, (1-sig2))\n",
    "\n",
    "        # for W1\n",
    "        sig1 = W1 @ sigmoid(x)\n",
    "        dsigda1 = np.outer(sig1, (1-sig1))        \n",
    "        \n",
    "        # update weights\n",
    "        W3_up = np.outer(dldg @ dgda3, a2)\n",
    "        W3 -= alpha * W3_up\n",
    "\n",
    "        W2_part1 = dldg @ dgda3 @ W3\n",
    "        W2_part2 = W2_part1 @ dsigda2\n",
    "        W2_up = np.outer(W2_part2, a1)\n",
    "        W2 -= alpha * W2_up\n",
    "        \n",
    "        W1_part1 = W2_part2 @ W2\n",
    "        W1_part2 = W1_part1 @ dsigda1\n",
    "        W1_up = np.outer(W1_part2, x)\n",
    "        W1 -= alpha * W1_up\n",
    "\n",
    "    # return weights\n",
    "    return W1, W2, W3\n",
    "\n",
    "test_x = np.array([np.ones(784)])\n",
    "#train_SGD(test_x, 784, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make training lists\n",
    "# train_x_list = []\n",
    "# train_y_list = []\n",
    "# for item in mnist_trainset:\n",
    "#     train_x_list.append(np.array(item[0].getdata()))\n",
    "#     y = np.zeros(10)\n",
    "#     y[item[1]] = 1\n",
    "#     train_y_list.append(y)\n",
    "\n",
    "# test_x_list = []\n",
    "# test_y_list = []\n",
    "# # make test lists\n",
    "# for item in mnist_testset:\n",
    "#     test_x_list.append(np.array(item[0].getdata()))\n",
    "#     y = np.zeros(10)\n",
    "#     y[item[1]] = 1\n",
    "#     test_y_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416.39388394355774\n"
     ]
    }
   ],
   "source": [
    "# ~ 4 sec / 1000 iters\n",
    "t0 = time.time()\n",
    "W1, W2, W3 = train_SGD(train_x_list, train_y_list, 784, 0.1, 1, 10**(5))\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.666666666666667"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10**(5) / 1000) * 4 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_NN(W1, W2, W3, test_x_list, test_y_list):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for i in range(len(test_x_list)):\n",
    "        x = test_x_list[i]\n",
    "        y_real = np.argmax(test_y_list[i])\n",
    "        y_pred = np.argmax(soft_max(W3 @ sigmoid(W2 @ sigmoid(W1 @ x))))\n",
    "        if y_real == y_pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    return correct / (correct + wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1028"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_NN(W1, W2, W3, test_x_list, test_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
